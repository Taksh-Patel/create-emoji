{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/50\n",
      "448/448 [==============================] - 487s 1s/step - loss: 1.7989 - accuracy: 0.2596 - val_loss: 1.7063 - val_accuracy: 0.3252\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 188s 419ms/step - loss: 1.6362 - accuracy: 0.3578 - val_loss: 1.5466 - val_accuracy: 0.4121\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 439s 981ms/step - loss: 1.5288 - accuracy: 0.4081 - val_loss: 1.4597 - val_accuracy: 0.4410\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 352s 786ms/step - loss: 1.4546 - accuracy: 0.4404 - val_loss: 1.4246 - val_accuracy: 0.4688\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 189s 423ms/step - loss: 1.3987 - accuracy: 0.4651 - val_loss: 1.3507 - val_accuracy: 0.4880\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 182s 407ms/step - loss: 1.3442 - accuracy: 0.4898 - val_loss: 1.3065 - val_accuracy: 0.5061\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 182s 406ms/step - loss: 1.3030 - accuracy: 0.5038 - val_loss: 1.2746 - val_accuracy: 0.5177\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 181s 403ms/step - loss: 1.2649 - accuracy: 0.5217 - val_loss: 1.2437 - val_accuracy: 0.5266\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 1.2271 - accuracy: 0.5393 - val_loss: 1.2196 - val_accuracy: 0.5366\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 1.1966 - accuracy: 0.5481 - val_loss: 1.2021 - val_accuracy: 0.5399\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 186s 415ms/step - loss: 1.1666 - accuracy: 0.5615 - val_loss: 1.1943 - val_accuracy: 0.5431\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 192s 428ms/step - loss: 1.1407 - accuracy: 0.5725 - val_loss: 1.1623 - val_accuracy: 0.5628\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 185s 414ms/step - loss: 1.1100 - accuracy: 0.5819 - val_loss: 1.1639 - val_accuracy: 0.5628\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 183s 408ms/step - loss: 1.0850 - accuracy: 0.5939 - val_loss: 1.1540 - val_accuracy: 0.5711\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 188s 419ms/step - loss: 1.0625 - accuracy: 0.6058 - val_loss: 1.1544 - val_accuracy: 0.5639\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 190s 425ms/step - loss: 1.0381 - accuracy: 0.6132 - val_loss: 1.1109 - val_accuracy: 0.5829\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 185s 413ms/step - loss: 1.0139 - accuracy: 0.6253 - val_loss: 1.1104 - val_accuracy: 0.5869\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.9933 - accuracy: 0.6330 - val_loss: 1.1087 - val_accuracy: 0.5833\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 185s 412ms/step - loss: 0.9658 - accuracy: 0.6431 - val_loss: 1.0997 - val_accuracy: 0.5911\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 183s 408ms/step - loss: 0.9404 - accuracy: 0.6534 - val_loss: 1.0955 - val_accuracy: 0.5890\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 182s 406ms/step - loss: 0.9190 - accuracy: 0.6607 - val_loss: 1.0861 - val_accuracy: 0.5956\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 184s 412ms/step - loss: 0.8974 - accuracy: 0.6707 - val_loss: 1.1032 - val_accuracy: 0.5908\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 183s 408ms/step - loss: 0.8724 - accuracy: 0.6821 - val_loss: 1.0794 - val_accuracy: 0.6031\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 180s 402ms/step - loss: 0.8461 - accuracy: 0.6890 - val_loss: 1.0724 - val_accuracy: 0.6069\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 182s 405ms/step - loss: 0.8213 - accuracy: 0.6994 - val_loss: 1.0857 - val_accuracy: 0.6013\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 185s 414ms/step - loss: 0.7985 - accuracy: 0.7094 - val_loss: 1.0823 - val_accuracy: 0.6052\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 188s 420ms/step - loss: 0.7837 - accuracy: 0.7109 - val_loss: 1.0834 - val_accuracy: 0.6044\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 186s 415ms/step - loss: 0.7634 - accuracy: 0.7197 - val_loss: 1.0882 - val_accuracy: 0.6092\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 193s 431ms/step - loss: 0.7342 - accuracy: 0.7335 - val_loss: 1.0814 - val_accuracy: 0.6080\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 188s 419ms/step - loss: 0.7055 - accuracy: 0.7425 - val_loss: 1.0953 - val_accuracy: 0.6062\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 185s 413ms/step - loss: 0.6813 - accuracy: 0.7536 - val_loss: 1.0948 - val_accuracy: 0.6136\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 184s 410ms/step - loss: 0.6705 - accuracy: 0.7564 - val_loss: 1.0936 - val_accuracy: 0.6148\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 201s 449ms/step - loss: 0.6424 - accuracy: 0.7667 - val_loss: 1.0998 - val_accuracy: 0.6182\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 200s 447ms/step - loss: 0.6185 - accuracy: 0.7758 - val_loss: 1.0993 - val_accuracy: 0.6232\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 194s 432ms/step - loss: 0.5984 - accuracy: 0.7800 - val_loss: 1.1136 - val_accuracy: 0.6183\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 200s 448ms/step - loss: 0.5853 - accuracy: 0.7908 - val_loss: 1.1232 - val_accuracy: 0.6182\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 187s 418ms/step - loss: 0.5664 - accuracy: 0.7949 - val_loss: 1.1171 - val_accuracy: 0.6182\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 195s 434ms/step - loss: 0.5394 - accuracy: 0.8064 - val_loss: 1.1420 - val_accuracy: 0.6169\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 185s 414ms/step - loss: 0.5238 - accuracy: 0.8117 - val_loss: 1.1451 - val_accuracy: 0.6116\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 187s 417ms/step - loss: 0.5092 - accuracy: 0.8160 - val_loss: 1.1452 - val_accuracy: 0.6150\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 193s 431ms/step - loss: 0.4991 - accuracy: 0.8197 - val_loss: 1.1357 - val_accuracy: 0.6229\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 191s 427ms/step - loss: 0.4729 - accuracy: 0.8300 - val_loss: 1.1731 - val_accuracy: 0.6214\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 185s 412ms/step - loss: 0.4639 - accuracy: 0.8327 - val_loss: 1.1647 - val_accuracy: 0.6222\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 197s 439ms/step - loss: 0.4482 - accuracy: 0.8372 - val_loss: 1.1746 - val_accuracy: 0.6166\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 209s 466ms/step - loss: 0.4332 - accuracy: 0.8455 - val_loss: 1.1870 - val_accuracy: 0.6235\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 193s 430ms/step - loss: 0.4189 - accuracy: 0.8484 - val_loss: 1.2063 - val_accuracy: 0.6180\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 189s 423ms/step - loss: 0.4063 - accuracy: 0.8545 - val_loss: 1.2087 - val_accuracy: 0.6251\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 201s 449ms/step - loss: 0.3944 - accuracy: 0.8576 - val_loss: 1.2045 - val_accuracy: 0.6208\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 188s 420ms/step - loss: 0.3823 - accuracy: 0.8613 - val_loss: 1.2244 - val_accuracy: 0.6173\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 207s 463ms/step - loss: 0.3682 - accuracy: 0.8689 - val_loss: 1.2578 - val_accuracy: 0.6184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'C:/Users/Taksh Patel/Downloads/data/train'\n",
    "val_dir = 'C:/Users/Taksh Patel/Downloads/data/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "# emotion_model.load_weights('emotion_model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "emotion_model.save_weights('C:/Users/Taksh Patel/Downloads/data/emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('C:/Users/Taksh Patel/Downloads/data/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('C:/Users/Taksh Patel/Downloads/data/emotion_model.h5')\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "emoji_dist={0:\"C:/Users/Taksh Patel/Downloads/data/emojis/angry.png\",1:\"C:/Users/Taksh Patel/Downloads/data/emojis/disgusted.png\",2:\"C:/Users/Taksh Patel/Downloads/data/emojis/fearfull.png\",3:\"C:/Users/Taksh Patel/Downloads/data/emojis/happy.png\",4:\"C:/Users/Taksh Patel/Downloads/data/emojis/neutral.png\",5:\"C:/Users/Taksh Patel/Downloads/data/emojis/sad.png\",6:\"C:/Users/Taksh Patel/Downloads/data/emojis/surprise.png\"}\n",
    "global last_frame1                                    \n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "def show_vid():      \n",
    "    cap1 = cv2.VideoCapture(0)                                 \n",
    "    if not cap1.isOpened():                             \n",
    "        print(\"cant open the camera1\")\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    bounding_box = cv2.CascadeClassifier('C:/Users/Taksh Patel/Downloads/data/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "    if flag1 is None:\n",
    "        print (\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)     \n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, show_vid)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "def show_vid2():\n",
    "    frame2=cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "    img2=Image.fromarray(frame2)\n",
    "    imgtk2=ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2=imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))\n",
    "    \n",
    "    lmain2.configure(image=imgtk2)\n",
    "    lmain2.after(10, show_vid2)\n",
    "if __name__ == '__main__':\n",
    "    root=tk.Toplevel()  \n",
    "    img = ImageTk.PhotoImage(Image.open(\"C:/Users/Taksh Patel/Downloads/data/logo.png\"))\n",
    "    heading = Label(root,image=img,bg='black')\n",
    "    \n",
    "    heading.pack() \n",
    "    heading2=Label(root,text=\"Photo to Emoji\",pady=20, font=('arial',18,'bold'),bg='black',fg='#CDCDCD')                                 \n",
    "    \n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    \n",
    "    root.title(\"Photo To Emoji\")            \n",
    "    root.geometry(\"1400x900+100+10\") \n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    show_vid()\n",
    "    show_vid2()\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
